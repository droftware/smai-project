{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import numpy.linalg as LA\n",
    "from sklearn import linear_model\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math\n",
    "# import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('./merged_2011_PP.csv', encoding='latin1', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### Handle NaN\n",
    "\n",
    "# Numerical : Replace with median\n",
    "for i in range(len(df.columns)):\n",
    "    typ= df[df.columns[i]].dtype\n",
    "    if( typ =='float' or typ =='int' or typ=='bool' ):\n",
    "        df[df.columns[i]].fillna(df[df.columns[i]].median(),inplace = True)\n",
    "\n",
    "# Categorical : With most frequent\n",
    "for i in range(len(df.columns)):\n",
    "    typ= df[df.columns[i]].dtype\n",
    "    if( typ =='object'):\n",
    "        df[df.columns[i]].fillna(df[df.columns[i]].value_counts().idxmax(),inplace = True)\n",
    "\n",
    "df=df.fillna(0) ## Filled all NaN with mean\n",
    "#### Handle Privacy Suppressed        \n",
    "df=df.replace(to_replace='PrivacySuppressed',value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Delete Non Informative Attributes (String type)\n",
    "del df['INSTNM']\n",
    "del df['CITY']\n",
    "del df['STABBR']\n",
    "del df['AccredAgency']\n",
    "del df['INSTURL']\n",
    "del df['NPCURL']\n",
    "del df['REPAY_DT_MDN'] ## del Date\n",
    "del df['SEPAR_DT_MDN'] ## del Date\n",
    "del df['ZIP']          ## Some others\n",
    "del df['UNITID']\n",
    "\n",
    "# df1=df[['DEP_INC_PCT_LO','PREDDEG','PCIP12','NPT41_PUB','NPT42_PUB','UGDS_ASIAN','FIRSTGEN_WDRAW_ORIG_YR3_RT','md_earn_wne_p6']]\n",
    "for i in range(len(df.columns)): ## Convert datatypes as integers from string\n",
    "    df[df.columns[i]]= pd.to_numeric(df[df.columns[i]])\n",
    "\n",
    "y= df['md_earn_wne_p6']\n",
    "del df['md_earn_wne_p6']\n",
    "\n",
    "y2=df['GRAD_DEBT_MDN']\n",
    "del df['GRAD_DEBT_MDN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "########Remove redundant features (manual selection) \n",
    "\n",
    "with open('to_remove.txt','r') as f: ## to_remove.txt contains list of features to remove\n",
    "    reader = f.read().splitlines()\n",
    "    bad_features=reader ## list of features that are to be removed \n",
    "    \n",
    "# bad_features.index('COMPL_RPY_1YR_RT')   \n",
    "for i in range(len(bad_features)):  #now removing them\n",
    "#     print(\"Removing\",bad_features[i],\"For i=\",i)\n",
    "    del df[bad_features[i]]         #del all bad features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['PREDDEG', 'HIGHDEG', 'CONTROL', 'ADM_RATE', 'SATVR25', 'SATVR75',\n",
      "       'SATMT25', 'SATMT75', 'SATWR25', 'SATWR75',\n",
      "       ...\n",
      "       'NOT1STGEN_WDRAW_2YR_TRANS_YR8_RT', 'NOT1STGEN_ENRL_4YR_TRANS_YR8_RT',\n",
      "       'NOT1STGEN_ENRL_2YR_TRANS_YR8_RT', 'NOT1STGEN_UNKN_ORIG_YR8_RT',\n",
      "       'NOT1STGEN_UNKN_4YR_TRANS_YR8_RT', 'NOT1STGEN_UNKN_2YR_TRANS_YR8_RT',\n",
      "       'DEP_INC_PCT_LO', 'PAR_ED_PCT_MS', 'PAR_ED_PCT_HS', 'PAR_ED_PCT_PS'],\n",
      "      dtype='object', length=458)\n"
     ]
    }
   ],
   "source": [
    "## TRIM USELESS FEATURES \n",
    "\n",
    "## Find out the features in which same value occurs many times\n",
    "copydf=df.copy(deep=True)\n",
    "# counter=0\n",
    "for i in range(len(copydf.columns)):\n",
    "    current=copydf[copydf.columns[i]]\n",
    "    typ=current.dtype\n",
    "    if(typ=='int64' or typ=='float64'):\n",
    "        count=current.value_counts()\n",
    "        count=list(count)\n",
    "        if(len(count)>0 and count[0]>7000):\n",
    "#                 counter+=1\n",
    "#             print(\"Attribute=\",df.columns[i],\"Freq=\",count[0])\n",
    "                del df[copydf.columns[i]]  #!!!!!!! should be df[copydf.columns[i]]\n",
    "\n",
    "print(df.columns) ## 458 left now\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x=df.copy(deep=True)\n",
    "\n",
    "## Train Data\n",
    "x_train=x[:6000]\n",
    "y_train=y[:6000]\n",
    "y_train=y_train.reshape(-1,1)\n",
    "y2_train=y2[:6000]\n",
    "y2_train=y2_train.reshape(-1,1)\n",
    "\n",
    "## Test Data\n",
    "x_test=x[6000:]\n",
    "y_test=y[6000:]\n",
    "y_test=y_test.reshape(-1,1)\n",
    "y2_test=y2[6000:]\n",
    "y2_test=y2_test.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "######## Dimensionality Reduction Techniques\n",
    "\n",
    "## Simple PCA\n",
    "# from sklearn.decomposition import PCA\n",
    "# pca = PCA(n_components=100)\n",
    "# pca.fit(df)\n",
    "# df=pca.transform(df)\n",
    "# df.shape\n",
    "\n",
    "# # Kernel PCA\n",
    "# from sklearn.decomposition import PCA, KernelPCA\n",
    "# kpca = KernelPCA(kernel=\"rbf\",gamma=10,n_components=400)\n",
    "# fit=kpca.fit(x_train)\n",
    "# x_train = kpca.transform(x_train)\n",
    "# x_test = kpca.transform(x_test)\n",
    "\n",
    "### Plot\n",
    "# plt.scatter(Z[:,0], Z[:,1])\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute % error =  12.3967424977\n",
      "Rmse= 8085.85120835\n"
     ]
    }
   ],
   "source": [
    "######## Linear Regression (Scikit--To check)\n",
    "############################################\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "regr.fit(x_train,y_train) ## Train\n",
    "result=regr.predict(x_test)  ## Test\n",
    "\n",
    "## Mean Absolute Percentage Error\n",
    "from numpy import inf\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    a=np.abs(y_true - y_pred)\n",
    "    a[a == inf] = 0\n",
    "    b=( a/ y_true)\n",
    "    b[b == inf] = 0\n",
    "    b=np.mean(b)\n",
    "    return b * 100\n",
    "\n",
    "percent_error=mean_absolute_percentage_error(y_test,result)\n",
    "print(\"Mean Absolute % error = \",percent_error)\n",
    "\n",
    "\n",
    "## RMSE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "RMSE = mean_squared_error(y_test, result)**0.5\n",
    "print(\"Rmse=\",RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Examine available columns\n",
    "#x=df[['DEP_INC_PCT_LO','PREDDEG','PCIP12','NPT41_PUB','NPT42_PUB','UGDS_ASIAN','FIRSTGEN_WDRAW_ORIG_YR3_RT']]\n",
    "#y=df[['md_earn_wne_p6']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "################## ALGORITHMS ##############################\n",
    "########### ############ ############ ############# ########\n",
    "####### ####### ######### ############### ########### #######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute % error =  15.734283616\n",
      "Rmse= 15717.5884852\n"
     ]
    }
   ],
   "source": [
    "##################### 1) LINEAR REGRESSION ################ \n",
    "###########################################################\n",
    "## Apply KPCA\n",
    "## Kernel PCA\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "kpca = KernelPCA(kernel=\"rbf\",gamma=10,n_components=400)\n",
    "fit=kpca.fit(x_train)\n",
    "x_train1 = kpca.transform(x_train)\n",
    "x_test1 = kpca.transform(x_test)\n",
    "\n",
    "# Linear Regression\n",
    "\n",
    "def augment(x):\n",
    "    numRows = x.shape[0]\n",
    "    onesVec = np.ones((numRows, 1))\n",
    "    y = np.append(onesVec, x, 1)\n",
    "    return y\n",
    "\n",
    "def normalize(X, mean_X, std_X):\n",
    "    '''\n",
    "        Normalize our sample set wrt to mean and standard deviation\n",
    "    '''\n",
    "    num_samples = X.shape[0]\n",
    "    for i in range(num_samples):\n",
    "        X[i,:] = X[i, :] - mean_X\n",
    "        X[i,:] = X[i, :]/std_X\n",
    "    return X\n",
    "        \n",
    "\n",
    "\n",
    "def lr_train(x_train, y_train, mean_X, std_X, alpha=0.005, epsilon = 0.002, num_iterations=500):\n",
    "    '''\n",
    "        Finding theta parameters for linear regression\n",
    "    '''\n",
    "    X = copy.deepcopy(x_train)\n",
    "    Y = copy.deepcopy(y_train)\n",
    "    X = normalize(X, mean_X, std_X)\n",
    "    X = augment(X)\n",
    "#     print('Dim of X', X.shape)\n",
    "    theta = np.ones(X.shape[1])\n",
    "    num_samples = X.shape[0]\n",
    "#     for k in range(num_iterations):\n",
    "    counter = 0\n",
    "    while True:\n",
    "        idx = random.randint(0, num_samples - 1)\n",
    "#         print('Dim of theta', theta.shape)\n",
    "#         print('Dim of X[idx]', X[idx].shape)\n",
    "        h = theta.dot(X[idx])\n",
    "        diff = Y[idx] - h\n",
    "\n",
    "        for j in range(theta.shape[0]):\n",
    "            theta[j] = theta[j] + alpha * diff * X[idx][j]\n",
    "#             print('thetaj:',j, ' -> ',theta[j])\n",
    "        error_value = alpha*diff\n",
    "        if error_value < 0:\n",
    "            error_value *= -1\n",
    "        if error_value < epsilon or counter > num_iterations:\n",
    "            break\n",
    "        counter += 1\n",
    "#         print('Counter: ', counter)\n",
    "    return theta\n",
    "\n",
    "def lr_test(x_test, theta, mean_X, std_X):\n",
    "    X = copy.deepcopy(x_test)\n",
    "    X = normalize(X, mean_X, std_X)\n",
    "#     print('x_normalize', X)\n",
    "    X = augment(X)\n",
    "#     print('x_augment', X)\n",
    "    num_samples = X.shape[0]\n",
    "    output = np.ones((num_samples, 1))\n",
    "    for i in range(num_samples):\n",
    "        idx = i\n",
    "#         print('theta', theta)\n",
    "#         print('X', X[idx])\n",
    "        output[idx] = theta.dot(X[idx])\n",
    "    return output\n",
    "\n",
    "import copy\n",
    "\n",
    "mean_X = np.mean(x_train1, 0)\n",
    "std_X = np.std(x_train1,0)\n",
    "\n",
    "theta = lr_train(x_train1, y_train, mean_X, std_X)\n",
    "# print('Theta', theta)\n",
    "# print('x_test', x_test)\n",
    "result = lr_test(x_test1, theta, mean_X, std_X)\n",
    "\n",
    "percent_error=mean_absolute_percentage_error(y_test,result)\n",
    "print(\"Mean Absolute % error = \",percent_error)\n",
    "\n",
    "## RMSE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "RMSE = mean_squared_error(y_test, result)**0.5\n",
    "print(\"Rmse=\",RMSE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133600"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train=np.array(x_train)\n",
    "x_test=np.array(x_test)\n",
    "np.max(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: Xtrain  (6000, 458) Shapes: Xtest (10, 458)\n",
      "result shape: (10,)\n",
      "0  :  21065.6965118\n",
      "1  :  18365.2812411\n",
      "2  :  21027.7131905\n",
      "3  :  27822.0592836\n",
      "4  :  18366.6571766\n",
      "5  :  21643.3143831\n",
      "6  :  17452.4692454\n",
      "7  :  26353.9469761\n",
      "8  :  38196.7184984\n",
      "9  :  38194.743035\n",
      "ytest shape, result (10, 1) (10,)\n",
      "Mean Absolute % error =  31.2729575358\n",
      "Y test [[20500]\n",
      " [19500]\n",
      " [19400]\n",
      " [25500]\n",
      " [16800]\n",
      " [23300]\n",
      " [29000]\n",
      " [24800]\n",
      " [38200]\n",
      " [38200]]\n",
      "Predicting Y [ 21065.69651181  18365.28124112  21027.71319049  27822.05928363\n",
      "  18366.65717655  21643.31438311  17452.46924544  26353.94697609\n",
      "  38196.71849843  38194.74303496]\n",
      "Rmse= 3880.82835011\n"
     ]
    }
   ],
   "source": [
    "############  2) WEIGHTED LINEAR REGRESSION ##############\n",
    "#########################################################\n",
    "def lwe_weight_matrix(X, point, c):\n",
    "    n = X.shape[0]\n",
    "    weight = np.eye(n)\n",
    "    for i in range(n):\n",
    "        diff = X[i] - point\n",
    "        diff_mod = LA.norm(diff)**2\n",
    "#         if i == 1:\n",
    "#             print('dif mod', diff_mod)\n",
    "#             print('Xi',X[i])\n",
    "#             print('point',point)\n",
    "        value = np.exp(-1*diff_mod*0.5/c**2)\n",
    "        weight[i, i] = value\n",
    "    return weight\n",
    "\n",
    "def lwr_test_point(x_train, y_train, point, c):\n",
    "#     X = copy.deepcopy(x_train)\n",
    "#     Y = copy.deepcopy(y_train)\n",
    "    X = x_train\n",
    "    Y = y_train\n",
    "    weight = lwe_weight_matrix(X, point,c)\n",
    "#     print(weight)\n",
    "    t1 = X.T.dot(weight)\n",
    "    t1 = t1.dot(X)\n",
    "    t1 = LA.inv(t1)\n",
    "    t2 = X.T.dot(weight)\n",
    "    t2 = t2.dot(Y)\n",
    "    beta = t1.dot(t2)\n",
    "    return point.dot(beta)\n",
    "\n",
    "def lwr_test(x_train, y_train, x_test, c=5):\n",
    "    print('Shapes: Xtrain ', x_train.shape, 'Shapes: Xtest', x_test.shape)\n",
    "    result = np.ones(x_test.shape[0])\n",
    "    X = copy.deepcopy(x_train)\n",
    "    Y = copy.deepcopy(y_train)\n",
    "    X_Test = copy.deepcopy(x_test)\n",
    "    \n",
    "    mean_X = np.mean(X,0)\n",
    "    std_X = np.std(X,0)\n",
    "    \n",
    "    X = normalize(X, mean_X, std_X)\n",
    "    X_Test = normalize(X_Test, mean_X, std_X)\n",
    "   \n",
    "    print('result shape:', result.shape)\n",
    "    for i in range(x_test.shape[0]):\n",
    "        result[i] = lwr_test_point(X, Y, X_Test[i], c)\n",
    "        print(i,' : ', result[i])\n",
    "#         print('For ', X_Test[i])\n",
    "    return result\n",
    "    \n",
    "\n",
    "# testing locally weighted linear regression\n",
    "ktil = 10\n",
    "result = lwr_test(x_train,y_train,x_test[0:ktil,:]) # tests point by point and not all at once\n",
    "\n",
    "print('ytest shape, result', y_test[0:ktil].shape, result.shape)\n",
    "percent_error=mean_absolute_percentage_error(y_test[0:ktil],result)\n",
    "print(\"Mean Absolute % error = \",percent_error)\n",
    "\n",
    "print('Y test', y_test[0:ktil])\n",
    "print('Predicting Y', result)\n",
    "## RMSE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "RMSE = mean_squared_error(y_test[0:ktil], result)**0.5\n",
    "print(\"Rmse=\",RMSE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute % error =  23.042434652\n",
      "Rmse= 13478.5492864\n"
     ]
    }
   ],
   "source": [
    "############## 3)  KNN Regression ################\n",
    "##################################################\n",
    "\n",
    "K=10\n",
    "train=df[:6000]\n",
    "test=df[6000:]\n",
    "test=test.as_matrix()\n",
    "train=train.as_matrix()\n",
    "\n",
    "result_knn=[]\n",
    "for i in range(0,len(test)): ## Take each test point and calculate ditance to each train point\n",
    "    D=[] ## The distance matrix\n",
    "    for j in range(0,len(train)): ## Distance to each train point\n",
    "        dist = np.linalg.norm(test[i]-train[j])\n",
    "        D.append(dist)\n",
    "    D=np.array(D)\n",
    "    indexes = D.argsort()[::1]\n",
    "    top_k = y[ indexes[0:K]] ## Will contain K nearest neighbours\n",
    "    \n",
    "    # Calculate the average of the prediction variable\n",
    "    price=0\n",
    "    for j in range(0,K):\n",
    "        price+=top_k.iloc[j]\n",
    "    price=price/K\n",
    "    result_knn.append(price)  \n",
    "\n",
    "## Mean Absolute Percentage Error\n",
    "from numpy import *\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    a=np.abs(y_true - y_pred)\n",
    "    where_are_NaNs = isnan(a)\n",
    "    a[where_are_NaNs] = 0\n",
    "#     a[a == inf] = 0\n",
    "    b=( a/ y_true)\n",
    "    b[b == inf] = 0\n",
    "    where_are_NaNs = isnan(b)\n",
    "    b[where_are_NaNs] = 0\n",
    "    b=np.mean(b)\n",
    "    return b * 100\n",
    "\n",
    "percent_error=mean_absolute_percentage_error(y_test,np.array(result_knn))\n",
    "print(\"Mean Absolute % error = \",percent_error)\n",
    "\n",
    "## RMSE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "RMSE = mean_squared_error(y_test, result_knn)**0.5\n",
    "print(\"Rmse=\",RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dim of weights (6000, 6000)\n",
      "Dim of y (1, 6000)\n",
      "[[ -4.50380725e-14]]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "Count:  10001\n",
      "Original-> [20500]  Predicted->  [ nan]\n",
      "Original-> [19500]  Predicted->  [ nan]\n",
      "Original-> [19400]  Predicted->  [ nan]\n",
      "Original-> [25500]  Predicted->  [ nan]\n",
      "Original-> [16800]  Predicted->  [ nan]\n",
      "Original-> [23300]  Predicted->  [ nan]\n",
      "Original-> [29000]  Predicted->  [ nan]\n",
      "Original-> [24800]  Predicted->  [ nan]\n",
      "Original-> [38200]  Predicted->  [ nan]\n",
      "Original-> [38200]  Predicted->  [ nan]\n",
      "Original-> [19200]  Predicted->  [ nan]\n",
      "Original-> [19400]  Predicted->  [ nan]\n",
      "Original-> [17700]  Predicted->  [ nan]\n",
      "Original-> [25300]  Predicted->  [ nan]\n",
      "Original-> [25700]  Predicted->  [ nan]\n",
      "Original-> [21200]  Predicted->  [ nan]\n",
      "Original-> [18600]  Predicted->  [ nan]\n",
      "Original-> [38200]  Predicted->  [ nan]\n",
      "Original-> [34100]  Predicted->  [ nan]\n",
      "Original-> [27800]  Predicted->  [ nan]\n",
      "Original-> [20700]  Predicted->  [ nan]\n",
      "Original-> [36100]  Predicted->  [ nan]\n",
      "Original-> [23200]  Predicted->  [ nan]\n",
      "Original-> [21100]  Predicted->  [ nan]\n",
      "Original-> [21100]  Predicted->  [ nan]\n",
      "Original-> [15200]  Predicted->  [ nan]\n",
      "Original-> [22800]  Predicted->  [ nan]\n",
      "Original-> [12800]  Predicted->  [ nan]\n",
      "Original-> [12800]  Predicted->  [ nan]\n",
      "Original-> [22300]  Predicted->  [ nan]\n",
      "Original-> [27200]  Predicted->  [ nan]\n",
      "Original-> [21900]  Predicted->  [ nan]\n",
      "Original-> [23600]  Predicted->  [ nan]\n",
      "Original-> [20000]  Predicted->  [ nan]\n",
      "Original-> [20000]  Predicted->  [ nan]\n",
      "Original-> [20000]  Predicted->  [ nan]\n",
      "Original-> [15200]  Predicted->  [ nan]\n",
      "Original-> [0]  Predicted->  [ nan]\n",
      "Original-> [14500]  Predicted->  [ nan]\n",
      "Original-> [57100]  Predicted->  [ nan]\n",
      "Original-> [29100]  Predicted->  [ nan]\n",
      "Original-> [38200]  Predicted->  [ nan]\n",
      "Original-> [20300]  Predicted->  [ nan]\n",
      "Original-> [0]  Predicted->  [ nan]\n",
      "Original-> [23900]  Predicted->  [ nan]\n",
      "Original-> [22200]  Predicted->  [ nan]\n",
      "Original-> [32900]  Predicted->  [ nan]\n",
      "Original-> [22000]  Predicted->  [ nan]\n",
      "Original-> [0]  Predicted->  [ nan]\n",
      "Original-> [35000]  Predicted->  [ nan]\n"
     ]
    }
   ],
   "source": [
    "################## 4) Neural Networks ###################\n",
    "#########################################################\n",
    "# Neural Nets\n",
    "\n",
    "def sigmoid(x):\n",
    "    k = 1\n",
    "    ex = math.e ** x\n",
    "    val = k*ex/(1 + ex)\n",
    "    return val\n",
    "\n",
    "def dif_sigmoid(x):\n",
    "    k = 1\n",
    "    sig = sigmoid(x)\n",
    "    val = k*sig * (1 - sig)\n",
    "    return val\n",
    "\n",
    "def relu(x):\n",
    "    return np.log(1+np.exp(x))\n",
    "\n",
    "def dif_relu(x):\n",
    "    return 1/(1+np.exp(-1.0*x))\n",
    "\n",
    "def activation(x):\n",
    "    return sigmoid(x)\n",
    "\n",
    "def dif_activation(x):\n",
    "    return dif_sigmoid(x)\n",
    "\n",
    "\n",
    "def feed_forward(x, wh, wo, nh, no):\n",
    "    y = []\n",
    "    z = []\n",
    "    neth = []\n",
    "    neto = []\n",
    "    aug_x = np.append(1, x)\n",
    "    for j in range(nh):\n",
    "        net_j = aug_x.dot(wh[j, :])\n",
    "        neth.append(net_j)\n",
    "#         y_j = sigmoid(net_j)\n",
    "        y_j = activation(net_j)\n",
    "        y.append(y_j)\n",
    "\n",
    "    yk = [1]\n",
    "    yk = yk + y\n",
    "    # y.insert(0, 1)\n",
    "    aug_y = np.array(yk)\n",
    "\n",
    "    for k in range(no):\n",
    "        # print 'aug_y: ', aug_y\n",
    "        # print 'wok: ', wo[k, :]\n",
    "        net_k = aug_y.dot(wo[k, :])\n",
    "        neto.append(net_k)\n",
    "        # print 'net_k', net_k\n",
    "#         z_k = sigmoid(net_k)\n",
    "        z_k = activation(net_k)\n",
    "        z.append(z_k)\n",
    "\n",
    "    z = np.array(z)\n",
    "    return z, y, neth, neto\n",
    "\n",
    "\n",
    "def stochastic_backprop(samples, outputs, d, nh, no, theta, eta):\n",
    "    random.seed(4)\n",
    "    wh = 2 * np.random.rand(nh, d+1) - 1\n",
    "    wo = 2 * np.random.rand(no, nh+1) - 1\n",
    "    num_samples = samples.shape[0]\n",
    "    # print 'Num samples: ', num_samples\n",
    "    count = 0\n",
    "    delo = []\n",
    "    delk = []\n",
    "\n",
    "    while True:\n",
    "        delo = []\n",
    "        idx = random.randint(0, num_samples - 1)\n",
    "        # digit = digits[idx]\n",
    "        # t = target_value(digit)\n",
    "        t = outputs[idx]\n",
    "        x = np.array(samples[idx])\n",
    "        z, y, neth, neto = feed_forward(x, wh, wo, nh, no)\n",
    "        aug_x = np.append(1, x)\n",
    "\n",
    "        yk = [1]\n",
    "        yk = yk + y\n",
    "        # y.insert(0, 1)\n",
    "        aug_y = np.array(yk)\n",
    "\n",
    "        # x = aug_x\n",
    "        for k in range(no):\n",
    "            dif = t[k] - z[k]\n",
    "#             fdash = dif_sigmoid(neto[k])\n",
    "            fdash = dif_activation(neto[k])\n",
    "            dell = dif * fdash\n",
    "            delo.append(dell)\n",
    "            for j in range(nh+1):\n",
    "                delta = eta * dell * aug_y[j]\n",
    "                wo[k, j] = wo[k, j] + delta\n",
    "\n",
    "        for j in range(nh):\n",
    "#             fdash = dif_sigmoid(neth[j])\n",
    "            fdash = dif_activation(neth[j])\n",
    "            sigma = 0\n",
    "            for k in range(no):\n",
    "                sigma += (wo[k, j] * delo[k])\n",
    "            dell = fdash * sigma\n",
    "            for i in range(d+1):                                                                         \n",
    "                delta = eta * dell * aug_x[i]\n",
    "                wh[j, i] = wh[j, i] + delta\n",
    "\n",
    "        z, y, neth, neto = feed_forward(x, wh, wo, nh, no)\n",
    "        diff = t - z\n",
    "#         print('dif', diff)\n",
    "        mod = np.linalg.norm(diff)\n",
    "        j = 0.5 * mod * mod\n",
    "        if count > 10000:\n",
    "            print(j)\n",
    "            break\n",
    "        # if count > 5000:\n",
    "        # \tbreak\n",
    "        count += 1\n",
    "\n",
    "    print('Count: ', count)\n",
    "\n",
    "    return wo,wh\n",
    "\n",
    "######\n",
    "\n",
    "#####\n",
    "\n",
    "def approx(x):\n",
    "    if x< 0 or x >1:\n",
    "        print('ERROR: Value approximated greater than 1: ', x)\n",
    "    if x < 0.5:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "\n",
    "def nn_train(X, Y):\n",
    "    d = X.shape[1]\n",
    "#     print('DDl', d)\n",
    "    nh = 8\n",
    "    no = Y.shape[1]\n",
    "    theta = 0.000001\n",
    "    eta = 0.125\n",
    "    wo, wh = stochastic_backprop(X, Y, d, nh, no, theta, eta)\n",
    "    return wo, wh, nh, no\n",
    "    \n",
    "def nn_test(point, wo, wh, nh, no):\n",
    "    z, y, neth, neto = feed_forward(point, wh, wo, nh, no)\n",
    "    return z\n",
    "\n",
    "\n",
    "wo, wh, nh, no = nn_train(x_train, y_train)\n",
    "\n",
    "for i in range(50):\n",
    "    result = nn_test(x_test[i], wo, wh, nh, no)\n",
    "    print('Original->', y_test[i], ' Predicted-> ', result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prakhar/Downloads/enter/lib/python3.5/site-packages/sklearn/svm/base.py:216: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return column_or_1d(y, warn=True).astype(np.float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted\n",
      "Predicted\n",
      "Mean Absolute % error =  17.068129199\n",
      "Rmse= 16783.402844\n"
     ]
    }
   ],
   "source": [
    "################## 5) SVM Regression ###################\n",
    "########################################################\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "clf = SVR(C=1.0, epsilon=0.2,kernel='linear')\n",
    "# SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.2, gamma='auto',\n",
    "#     kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n",
    "clf.fit(x_train,y_train.tolist())\n",
    "print('Fitted')\n",
    "result=clf.predict(x_test)\n",
    "print('Predicted')\n",
    "\n",
    "## Mean Absolute Percentage Error\n",
    "from numpy import inf\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    a=np.abs(y_true - y_pred)\n",
    "    a[a == inf] = 0\n",
    "    b=( a/ y_true)\n",
    "    b[b == inf] = 0\n",
    "    b=np.mean(b)\n",
    "    return b * 100\n",
    "\n",
    "percent_error=mean_absolute_percentage_error(y_test,result)\n",
    "print(\"Mean Absolute % error = \",percent_error)\n",
    "\n",
    "\n",
    "## RMSE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "RMSE = mean_squared_error(y_test, result)**0.5\n",
    "print(\"Rmse=\",RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##########               ########\n",
    "########## PLOtting Code ########\n",
    "##########              ########\n",
    "\n",
    "#########3 Correlation List\n",
    "# copydf=df.copy(deep=True)\n",
    "# copydf.loc[:,copydf.shape[1]]=y \n",
    "# corr = copydf.select_dtypes(include = ['float64', 'int64']).iloc[:, 1:].corr()\n",
    "# a=[]\n",
    "# cor_dict = corr[copydf.shape[1]-1].to_dict()\n",
    "# print(\"List the numerical features decendingly by their correlation with Earnings:\\n\")\n",
    "# for ele in sorted(cor_dict.items(), key = lambda x: -abs(x[1])):\n",
    "# #     print(\"{0}:\\t\\t\\t\\t\\t\\t\\t{1}\".format(*ele))\n",
    "#         a.append([ele[0],ele[1]])\n",
    "# b=pd.DataFrame(a)\n",
    "# b\n",
    "\n",
    "########## Graph and General info of Prediction Variables\n",
    "# print(\"Some Statistics of the Income:\\n\")\n",
    "# print(y2.describe())\n",
    "# sns.distplot(y2, kde = False, color = 'b', hist_kws={'alpha': 0.9})\n",
    "\n",
    "\n",
    "########## HEAT MAP #############\n",
    "# copydf=df.as_matrix()\n",
    "# copydf=copydf[:,:20]\n",
    "# copydf=pd.DataFrame(copydf)\n",
    "# copydf.loc[:,copydf.shape[1]]=y\n",
    "# corr = copydf.select_dtypes(include = ['float64', 'int64']).iloc[:, 1:].corr()\n",
    "# plt.figure(figsize=(12, 12))\n",
    "# sns.heatmap(corr, vmax=1, square=True)\n",
    "### 20 is our median earning\n",
    "\n",
    "########## Correlation\n",
    "# sns.regplot(x=df['PCIP45'], y=y, data = df, color = 'Orange')\n",
    "\n",
    "########## Asian and Predominant degree awarded\n",
    "# fig = plt.figure(2, figsize=(9, 7))\n",
    "# plt.subplot(211)\n",
    "# plt.scatter(df.UGDS_ASIAN.values,y)\n",
    "# plt.title('Asian Undergrads')\n",
    "# plt.subplot(212)\n",
    "# plt.scatter(df.PREDDEG.values, y)\n",
    "# plt.title('Predominant Degree')\n",
    "# fig.text(-0.01, 0.5, y, va = 'center', rotation = 'vertical', fontsize = 12)\n",
    "# plt.tight_layout()\n",
    "\n",
    "\n",
    "############# OLD HEAT MAP\n",
    "# # Heat Map\n",
    "# sns.set(style=\"white\")\n",
    "# # df1=df[['DEP_INC_PCT_LO','PREDDEG','PCIP12','NPT41_PUB','NPT42_PUB','UGDS_ASIAN','FIRSTGEN_WDRAW_ORIG_YR3_RT','md_earn_wne_p6']]\n",
    "# # Compute the correlation matrix\n",
    "# corr = df1.corr()\n",
    "# # print(corr)\n",
    "# # Generate a mask for the upper triangle\n",
    "# mask = np.zeros_like(corr, dtype=np.bool)\n",
    "# mask[np.triu_indices_from(mask)] = True\n",
    "# # Set up the matplotlib figure\n",
    "# f, ax = plt.subplots(figsize=(11, 9))\n",
    "# # Generate a custom diverging colormap\n",
    "# cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "# # Draw the heatmap with the mask and correct aspect ratio\n",
    "# sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3,\n",
    "#             square=True, xticklabels=5, yticklabels=5,\n",
    "#             linewidths=.5, cbar_kws={\"shrink\": .5}, ax=ax)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
